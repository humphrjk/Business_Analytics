{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries\n",
    "First, ensure you have statsmodels installed, or you can install it using pip (pip install statsmodels). Then, import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load and Prepare the Data\n",
    "Load your dataset and separate your features (independent variables) and target (dependent variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('path/to/your/data.csv')\n",
    "\n",
    "df=pd.read_csv('path/to/your/data.csv')\n",
    "df.head()\n",
    "list(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X1', 'X2', ..., 'Xn' are your feature columns and 'Y' is your target variable\n",
    "X = df[['X1', 'X2', 'Xn']]\n",
    "Y = df['Y']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create and Fit the Model\n",
    "Create the model using OLS (Ordinary Least Squares) from statsmodels, fit it, and then view the summary for detailed statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# Use if you want to use a linear model\n",
    "model = sm.OLS(Y, X)\n",
    "\n",
    "# Use if you want to use a logistic model\n",
    "#model = sm.Logit(Y, X)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the VIF factor to determine if there is multicollinearity in you model\n",
    "#### Interpreting VIF Values:\n",
    "VIF = 1: No correlation between the independent variable and the other variables.<br>\n",
    "VIF < 5: Generally considered okay, indicating moderate correlation that may not require action.<br>\n",
    "VIF >= 5 to 10: Indicates high correlation that may distort the reliability of the coefficient of this variable and should be examined for potential removal or adjustment.<br>\n",
    "VIF > 10: Suggests extreme multicollinearity. Variables with such high VIFs are typically removed from the model, or the model is re-specified to reduce multicollinearity.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming X is the DataFrame of predictors including the constant\u001b[39;00m\n\u001b[0;32m      4\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 5\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      6\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [variance_inflation_factor(X\u001b[38;5;241m.\u001b[39mvalues, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(vif_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming X is the DataFrame of predictors including the constant\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extracting Model Metrics\n",
    "While the summary provides a comprehensive overview, including p-values, R-squared, and coefficients, you might want to extract specific metrics or values into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "residuals = Y - results.fittedvalues\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(results.fittedvalues, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.show()\n",
    "\n",
    "# Checking for normality of residuals\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Creating a DataFrame for coefficients and p-values\n",
    "coefficients = results.params\n",
    "p_values = results.pvalues\n",
    "mse = mean_squared_error(Y, results.fittedvalues)\n",
    "\n",
    "model_metrics = pd.DataFrame({'Coefficient': coefficients, 'P-Value': p_values})\n",
    "model_metrics['MSE'] = mse\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "Ensure to replace 'path/to/your/data.csv', 'X1', 'X2', 'Xn', and 'Y' with actual column names from your dataset.\n",
    "This approach provides a constant term (intercept) in the model, which is generally recommended in regression analysis.\n",
    "statsmodels' OLS does not automatically split the data into training and testing sets since it's often used for statistical analysis rather than predictive modeling, aligning with your requirement.\n",
    "This setup gives you a detailed statistical overview of your regression model, focusing on survey analysis, and extracts key metrics and estimates for further examination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For evaluating the quality of a linear regression model, especially in contexts like survey analysis where interpretation and understanding of model behavior are paramount, there are several key metrics and statistical tests you can consider in addition to the Mean Squared Error (MSE) and p-values:\n",
    "\n",
    "### 1. R-squared (Coefficient of Determination):\n",
    "What it is: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "How to interpret: Values range from 0 to 1. A higher R-squared value indicates a better fit between the model and the observed data.\n",
    "### 2. Adjusted R-squared:\n",
    "What it is: Adjusts the R-squared value based on the number of predictors in the model relative to the number of observations. It accounts for the model complexity.\n",
    "How to interpret: Like R-squared, but provides a more accurate measure in the context of multiple predictors by penalizing the addition of non-significant predictors.\n",
    "### 3. F-statistic and its associated p-value:\n",
    "What it is: Tests the overall significance of the model.\n",
    "How to interpret: The F-statistic tests whether at least one predictor variable has a non-zero coefficient. A very low p-value (typically <.05) indicates that your model is significantly different from a model with no independent variables.\n",
    "### 4. AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion):\n",
    "What they are: These are information criteria metrics that evaluate the model fit while penalizing the model complexity (number of predictors).\n",
    "How to interpret: They help in model selection. Lower values indicate a better model, by balancing goodness of fit and complexity.\n",
    "### 5. Residual Analysis:\n",
    "What it is: Examination of the residuals (the differences between observed and predicted values) can provide insights into the model's accuracy and assumptions (like homoscedasticity and normality).\n",
    " \n",
    "How to perform and interpret:\n",
    "Plotting residuals vs. predicted values: Should show no clear pattern.\n",
    "Normality test of residuals: E.g., using a Q-Q plot or statistical tests. Residuals should ideally follow a normal distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
